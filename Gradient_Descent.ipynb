{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0970ece05f2fd367a433285c0204e778ad1644066d163bed046b3b0abfdd35b59",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed packages/libraries/modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, datasets, preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the inbuilt boston dataset\n",
    "\n",
    "data=datasets.load_boston()\n",
    "\n",
    "# getting the input, output and feature names\n",
    "\n",
    "X=data.data\n",
    "Y=data.target\n",
    "columns=data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677083   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n      <td>506.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>3.613524</td>\n      <td>11.363636</td>\n      <td>11.136779</td>\n      <td>0.069170</td>\n      <td>0.554695</td>\n      <td>6.284634</td>\n      <td>68.574901</td>\n      <td>3.795043</td>\n      <td>9.549407</td>\n      <td>408.237154</td>\n      <td>18.455534</td>\n      <td>356.674032</td>\n      <td>12.653063</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.601545</td>\n      <td>23.322453</td>\n      <td>6.860353</td>\n      <td>0.253994</td>\n      <td>0.115878</td>\n      <td>0.702617</td>\n      <td>28.148861</td>\n      <td>2.105710</td>\n      <td>8.707259</td>\n      <td>168.537116</td>\n      <td>2.164946</td>\n      <td>91.294864</td>\n      <td>7.141062</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.006320</td>\n      <td>0.000000</td>\n      <td>0.460000</td>\n      <td>0.000000</td>\n      <td>0.385000</td>\n      <td>3.561000</td>\n      <td>2.900000</td>\n      <td>1.129600</td>\n      <td>1.000000</td>\n      <td>187.000000</td>\n      <td>12.600000</td>\n      <td>0.320000</td>\n      <td>1.730000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.082045</td>\n      <td>0.000000</td>\n      <td>5.190000</td>\n      <td>0.000000</td>\n      <td>0.449000</td>\n      <td>5.885500</td>\n      <td>45.025000</td>\n      <td>2.100175</td>\n      <td>4.000000</td>\n      <td>279.000000</td>\n      <td>17.400000</td>\n      <td>375.377500</td>\n      <td>6.950000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.256510</td>\n      <td>0.000000</td>\n      <td>9.690000</td>\n      <td>0.000000</td>\n      <td>0.538000</td>\n      <td>6.208500</td>\n      <td>77.500000</td>\n      <td>3.207450</td>\n      <td>5.000000</td>\n      <td>330.000000</td>\n      <td>19.050000</td>\n      <td>391.440000</td>\n      <td>11.360000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.677083</td>\n      <td>12.500000</td>\n      <td>18.100000</td>\n      <td>0.000000</td>\n      <td>0.624000</td>\n      <td>6.623500</td>\n      <td>94.075000</td>\n      <td>5.188425</td>\n      <td>24.000000</td>\n      <td>666.000000</td>\n      <td>20.200000</td>\n      <td>396.225000</td>\n      <td>16.955000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>88.976200</td>\n      <td>100.000000</td>\n      <td>27.740000</td>\n      <td>1.000000</td>\n      <td>0.871000</td>\n      <td>8.780000</td>\n      <td>100.000000</td>\n      <td>12.126500</td>\n      <td>24.000000</td>\n      <td>711.000000</td>\n      <td>22.000000</td>\n      <td>396.900000</td>\n      <td>37.970000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# loading the data into a dataframe and then describing it to analyze it better \n",
    "\n",
    "df=pd.DataFrame(X, columns=columns)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the input data because of the different ranges of the features and we're standardising it beacause of the presence of outliers in the different features \n",
    "\n",
    "scaler=pp.StandardScaler()\n",
    "\n",
    "X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing \n",
    "\n",
    "X_train, X_test, Y_train, Y_test=model_selection.train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score function which gives the coeffient of determination \n",
    "\n",
    "def score(Y_pred, Y_true):\n",
    "    np_y_pred=np.array(Y_pred)\n",
    "    np_y_true=np.array(Y_true)\n",
    "    \n",
    "    u=((np_y_true-np_y_pred)**2).sum()\n",
    "    v=((np_y_true-np_y_true.mean())**2).sum()\n",
    "    \n",
    "    score=(1-(u/v))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function to predict the output using the best fit line found out by gradient descent \n",
    "\n",
    "def predict(x, coeffs):\n",
    "    M=x.shape[0]\n",
    "    N=x.shape[1]\n",
    "\n",
    "    y_pred=[0 for i in range(M)]        \n",
    "\n",
    "    for i in range(M):\n",
    "        sum=0\n",
    "\n",
    "        for j in range(N):\n",
    "            xij=x[i][j]\n",
    "            mj=coeffs[j]\n",
    "\n",
    "            sum+=(xij*mj)\n",
    "\n",
    "        sum+=(coeffs[N])\n",
    "    \n",
    "        y_pred[i]=sum\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step gradient function which makes change in the regression coefficents(m1, m2 till mn+1(c)) and makes us move one step closer to the optimal value of the cost function using a given learning rate \n",
    "\n",
    "def step_gradient(learning_rate, coeffs):\n",
    "    M=(X_train.shape[0])\n",
    "    N=len(columns)\n",
    "\n",
    "    slope=[0 for i in range(N+1)]   \n",
    "\n",
    "    for k in range(N+1):\n",
    "        sum1=0\n",
    "\n",
    "        for i in range(M):      \n",
    "            sum2=0\n",
    "\n",
    "            for j in range(N):\n",
    "                xij=X_train[i][j]\n",
    "                mj=coeffs[j]\n",
    "\n",
    "                sum2+=(xij*mj)\n",
    "\n",
    "            sum2+=coeffs[N]\n",
    "            sum2*=-1\n",
    "            \n",
    "            yi=Y_train[i]\n",
    "\n",
    "            sum2+=yi  \n",
    "            \n",
    "            xik=None\n",
    "\n",
    "            if k==N:\n",
    "                xik=1\n",
    "            else:\n",
    "                xik=X_train[i][k]\n",
    "                                  \n",
    "            sum2*=xik\n",
    "            sum1+=sum2\n",
    "                \n",
    "        sum1/=(-2/M)\n",
    "\n",
    "        slope[k]=sum1    \n",
    "\n",
    "    np_coeffs=np.array(coeffs, dtype=float)\n",
    "    np_slope=np.array(slope) \n",
    "\n",
    "    np_coeffs-=(learning_rate*np_slope)\n",
    "\n",
    "    return np_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function which finds the cost for a particular set of coefficients(m1, m2 till mn+1(c)) and we need to minmise this cost function to get the optimal values of the regression coefficients using the gradient descent algorithm \n",
    "\n",
    "def cost(coeffs):\n",
    "    cost=0\n",
    "    \n",
    "    M=X_train.shape[0]\n",
    "    N=len(columns)\n",
    "\n",
    "    for i in range(M):        \n",
    "        sum=0\n",
    "\n",
    "        for j in range(N):\n",
    "            xij=X_train[i][j]\n",
    "            mj=coeffs[j]\n",
    "\n",
    "            sum+=(mj*xij)\n",
    "\n",
    "        sum+=(coeffs[N])\n",
    "        sum*=-1\n",
    "\n",
    "        yi=Y_train[i]  \n",
    "\n",
    "        sum+=yi\n",
    "        sum**=2\n",
    "\n",
    "        cost+=sum\n",
    "    \n",
    "    cost/=M\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd function which implements the gradient descent algorithm \n",
    "\n",
    "def gd(learning_rate):\n",
    "    N=len(columns)\n",
    "    coeffs=[1 for i in range(N+1)]\n",
    "\n",
    "    # finding the learning rate value for which we just don't overshoot and the cost begins to decrease\n",
    "\n",
    "    prev_cost=cost(coeffs)  \n",
    "\n",
    "    while True:\n",
    "        new_coeffs=step_gradient(learning_rate, coeffs)\n",
    "\n",
    "        new_cost=cost(new_coeffs)\n",
    "\n",
    "        if new_cost>=prev_cost:\n",
    "            learning_rate/=10\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    i=0\n",
    "\n",
    "    while True:\n",
    "        prev_coeffs=coeffs\n",
    "        prev_cost=cost(coeffs)\n",
    "\n",
    "        coeffs=step_gradient(learning_rate, coeffs)\n",
    "\n",
    "        new_cost=cost(coeffs)\n",
    "\n",
    "        # if the cost increases at some point, then we simply reduce the learning rate, reset coeffs to previous value and then repeat the process(this is the case when the learning rate choosen in the above loop takes us to the other side of the parabola(wrt axis) while descreasing the cost(in the case of single feature input))\n",
    "\n",
    "        if new_cost>=prev_cost:\n",
    "            learning_rate/=10\n",
    "            coeffs=prev_coeffs\n",
    "\n",
    "            continue    \n",
    "\n",
    "        # if the absolute difference between the new and prev cost is <= 0.01(value considered after analyzing the decrease trend in the cost values), then we simply break\n",
    "\n",
    "        if abs(new_cost-prev_cost)<=0.01:\n",
    "            break        \n",
    "\n",
    "        i+=1\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function to run the gradient descent algorithm and get the optimal coefficients \n",
    "\n",
    "def run():\n",
    "    # initial value of learning rate \n",
    "\n",
    "    learning_rate=0.1\n",
    "\n",
    "    coeffs=gd(learning_rate)\n",
    "\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting the optimal coefficients by running the gradient descent algorithm\n",
    "\n",
    "coeffs=run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions for the test data\n",
    "\n",
    "Y_pred=predict(X_test, coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the predictions in a csv file \n",
    "\n",
    "df=pd.DataFrame(Y_pred)\n",
    "\n",
    "df.to_csv(\"predictions.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7829622528971131\n"
     ]
    }
   ],
   "source": [
    "# getting the score for the gradient descent algorithm \n",
    "\n",
    "score=score(Y_pred, Y_test)\n",
    "\n",
    "print(score)"
   ]
  }
 ]
}